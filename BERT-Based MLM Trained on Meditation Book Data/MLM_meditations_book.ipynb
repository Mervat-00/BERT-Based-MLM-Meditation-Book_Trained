{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20725bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer , BertForMaskedLM\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c450323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a602a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open book and read the text data\n",
    "with open('clean.txt','r') as f:\n",
    "    text = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c2ca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69bca36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From my grandfather Verus I learned good morals and the government of my temper.',\n",
       " 'From the reputation and remembrance of my father, modesty and a manly character.',\n",
       " 'From my mother, piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.',\n",
       " 'From my great-grandfather, not to have frequented public schools, and to have had good teachers at home, and to know that on such things a man should spend liberally.',\n",
       " \"From my governor, to be neither of the green nor of the blue party at the games in the Circus, nor a partizan either of the Parmularius or the Scutarius at the gladiators' fights; from him too I learned endurance of labour, and to want little, and to work with my own hands, and not to meddle with other people's affairs, and not to be ready to listen to slander.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b183e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing text with parameters suitable for the used bert version \n",
    "# - return_tensors=\"pt\" -->returns tokenized text as pytorch tensors\n",
    "# - max_length = 512 --> sets the sequence lenght as 512 as the 'bert-base-uncased' model handle inputs up to 512 tokens in length.\n",
    "# - padding = 'max_length'-->If the sequence is shorter than the required maximum sequence length, padding is added (commonly using a special [PAD] token) to extend the sequence to the required length.\n",
    "# - truncation = True --> If the sequence is longer than the maximum allowed length, it is truncated to the maximum length to ensure it fits the model's input size requirement.\n",
    "inputs = tokenizer(text , return_tensors = \"pt\" , max_length = 512 , padding = 'max_length' ,truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2748abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2013,  2026,  ...,     0,     0,     0],\n",
       "        [  101,  2013,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2013,  2026,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3459,  2185,  ...,     0,     0,     0],\n",
       "        [  101,  2043, 15223,  ...,     0,     0,     0],\n",
       "        [  101,  7887,  3288,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a315f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cloning the input ids in order to apply masking without changing the original data \n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aefe302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1394, 0.4252, 0.5837,  ..., 0.4468, 0.0633, 0.8044],\n",
       "        [0.3702, 0.2954, 0.4539,  ..., 0.0396, 0.9964, 0.6642],\n",
       "        [0.0957, 0.1077, 0.7318,  ..., 0.7677, 0.8156, 0.7363],\n",
       "        ...,\n",
       "        [0.0503, 0.0248, 0.1981,  ..., 0.5050, 0.8080, 0.2994],\n",
       "        [0.9397, 0.9986, 0.4246,  ..., 0.7890, 0.1979, 0.3807],\n",
       "        [0.0980, 0.6100, 0.3070,  ..., 0.4779, 0.1429, 0.8417]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''create random list of floats between [0,1] , (uniform distribution) with same size of input_ids ,\n",
    "in order to define later the selected 15% tokens to mask'''\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa1b309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False,  True, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#masking tokens with 15% probability , excepting CLS --> 101 , SEP --> 102 AND PAD -->0\n",
    "masks = (rand < .15) & (inputs.input_ids != 101)& (inputs.input_ids != 102)& (inputs.input_ids != 0)\n",
    "masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506b559",
   "metadata": {},
   "source": [
    "- each True value represents a token to be masked , now , we get the indices of each True value within each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68030ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([507, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e5c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting indices\n",
    "indices = []\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    indices.append(\n",
    "    torch.flatten(masks[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94fbe210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15],\n",
       " [8, 15],\n",
       " [1, 8, 14, 26, 29, 30, 32, 34],\n",
       " [6, 9, 10, 12, 13, 18, 31],\n",
       " [1, 12, 17, 33, 35, 51, 53, 60, 64, 72]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1af91274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2013,  2026,  ...,     0,     0,     0],\n",
       "        [  101,  2013,  1996,  ...,     0,     0,     0],\n",
       "        [  101,   103,  2026,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   103,  2185,  ...,     0,     0,     0],\n",
       "        [  101,  2043, 15223,  ...,     0,     0,     0],\n",
       "        [  101,  7887,  3288,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the input ids of the previous indices with mask id 103\n",
    "for i in range (inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, indices[i]] = 103\n",
    "    \n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c2fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the inputs into torch dataset for better processing \n",
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "#initializing the data \n",
    "data = MeditationsDataset(inputs)\n",
    "#intializing data loader which is used to load our data into the model \n",
    "data_loader = torch.utils.data.DataLoader(data , batch_size=10 , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73fe81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up resources \n",
    "#checking wether or not we have accessibility to GPU , if not use CPU \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9bf0a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make the model work on the available device \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a20a995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "#activate the training mode for model\n",
    "model.train()\n",
    "#setup optimizer\n",
    "#lr=5e-5: This specifies the learning rate of the optimizer, set to 0.00005 (5e-5). \n",
    "optimizer = AdamW(model.parameters(), lr =5e-5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "917113b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████| 51/51 [15:10<00:00, 17.85s/it, loss=0.158]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████| 51/51 [14:59<00:00, 17.64s/it, loss=0.134]\n"
     ]
    }
   ],
   "source": [
    "#import tqdm for creating progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "#set the number of training epochs\n",
    "epochs = 2\n",
    "\n",
    "#loop over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    #set up the progress bar for tracking the training loop\n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    #iterate over each batch of data\n",
    "    for batch in loop:\n",
    "        #clear any previously calculated gradients before performing a backward pass,to avoid accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "        #load 'input_ids' from the batch to the specified device (GPU/CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        #load 'attention_mask' from the batch to the specified device\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #load 'labels' from the batch to the specified device\n",
    "        labels = batch['labels'].to(device)\n",
    "        #forward pass: Compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        #retrieve the loss from model outputs\n",
    "        loss = outputs.loss\n",
    "        #perform backpropagation: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        #perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        #update the progress bar with epoch number and current loss\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
